---
title: "State Space Models for Longitudinal Neuropsychological Outcomes"
author: Zach Baucom
output:
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{amsmath}
- \newcommand\mysim{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny asym}}}{\sim}}}
- \usepackage{algorithm,algorithmic}
---

# Recap of Project 1

## The model 

We wish to model the data according to a specific SSM, the Local Linear Trend Model (LLT),



\begin{align*}
y_{ij} &= \alpha_{ij} + x_{ij}^T\beta
+ \varepsilon_{ij}\\
\alpha_{ij}&= \alpha_{i(j-1)} + \eta_{ij} 
\end{align*}



Where $\alpha_0 \sim N(a_0, P_0)$, $\varepsilon_{ij} \sim N(0, \sigma^2_\varepsilon)$, and $\eta_{ij} \sim N(0, \delta_{ij}\sigma^2_\eta)$.

* $y_j$ is an $n \times 1$ observation vector where $n$ indicates the number of subjects.
* $\alpha_j$ is an $n \times 1$ latent state vector.
  * Variation in $\alpha_j$ over time creates a dynamic moving average auto-correlation between observations $y_j$.
* $X_j$ is an $n \times p$ matrix of time varying covariates.
* $\beta$ is the linear effect of the columns in $X_j$.

## Results

* The LLT models did a much better job at measuring linear effects than the commonly used linear mixed effect models.
* Among the LLTs, the Bayesian Estimation Process yielded the best results in terms of accuracy and computation time.
* Paper 1 is currently under review at the Annals of Applied Statistics.



```{r, include = FALSE}
#https://pubmed.ncbi.nlm.nih.gov/21606904/#&gid=article-figures&pid=figure-1-uid-0

library(gridExtra)
library(lme4)
library(tidyverse)
library(knitr)
library(kableExtra)
library(latex2exp)
library(abind)
library(nlme)
### Set working directory to correct location
# cdir <- stringr::str_split(getwd(), "/")[[1]]
# udir <- cdir[1:which(cdir == "State-Space-Methods")]
# knitr::opts_knit$set(root.dir = paste(udir, collapse = "/"))

fct_case_when <- function(...) {
  args <- as.list(match.call())
  levels <- sapply(args[-1], function(f) f[[3]])  # extract RHS of formula
  levels <- levels[!is.na(levels)]
  factor(dplyr::case_when(...), levels=levels)
}
knitr::opts_chunk$set(echo = FALSE, dev = "pdf", message = FALSE, warning = FALSE)
```

# Project 2 Multivatiate Models

## Aims of Project 2

* Cognitive tests, offered by the NACC, are administered to measure different underlying constructs of cognition.
  * e.g. memory, attention, executive, and language.
* Understanding how the tests are related to different constructs is vital for interpreting effects of interest.
* In order to create a model capable of accounting for inter-relatedness between tests, we propose a Multivariate Bayesian Local Linear Trend Model.

## Aims

* With the Multivariate Bayesian Local Linear Trend Model we wish to:
  * Gain power by taking advantage of correlations between tests.
  * Get insight into relatedness of underlying cognition levels from each test.
  * Be able to compare linear effects across tests (i.e. does APOE have the same effect for test A and test B?)

## The Model


\begin{equation*}
\begin{aligned}
\begin{bmatrix}
y_{ij1}\\
y_{ij2}\\
\vdots\\
y_{ijK}
\end{bmatrix}
&= \begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\vdots\\
\alpha_{ijK}
\end{bmatrix}
+ 
\begin{bmatrix}
\boldsymbol{x_{ij}\beta_1}\\
\boldsymbol{x_{ij}\beta_2}\\
\vdots\\
\boldsymbol{x_{ij}\beta_K} 
\end{bmatrix} +
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\vdots\\
\varepsilon_{ijK}
\end{bmatrix},  
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\vdots\\
\varepsilon_{ijK}
\end{bmatrix} 
\sim N(0, \Sigma_\varepsilon
)\\
\begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\vdots\\
\alpha_{ijK}
\end{bmatrix} & = 
\begin{bmatrix}
\alpha_{i(j-1)1}\\
\alpha_{i(j-1)2}\\
\vdots\\
\alpha_{i(j-1)K}
\end{bmatrix} +
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\vdots\\
\eta_{ijK}
\end{bmatrix}, 
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\vdots\\
\eta_{ijK}
\end{bmatrix} \sim N(0, \delta_{ij}\Sigma_\eta)
\end{aligned}
\end{equation*}


## Model 1: Correlation is in Measurement eq.

\begin{equation*}
\begin{aligned}
\Sigma_\varepsilon =
\begin{bmatrix}
\sigma^{2(1)}_\varepsilon & \sigma^{2(1,2)}_\varepsilon & \cdots & \sigma^{2(1, K)}_\varepsilon\\
\sigma^{2(1,2)}_\varepsilon & \sigma^{2(2)}_\varepsilon\\
\vdots & & \ddots\\
\sigma^{2(1, K)}_\varepsilon & & &\sigma^{2(K)}_\varepsilon
\end{bmatrix},
 \Sigma_\eta = 
\begin{bmatrix}
\sigma^{2(1)}_\eta & 0 & \cdots & 0\\
0 & \sigma^{2(2)}_\eta\\
\vdots & & \ddots\\
0 & & &\sigma^{2(K)}_\eta
\end{bmatrix}
\end{aligned}
\end{equation*}


## Model 2: Correlation is in State eq.


\begin{equation*}
\begin{aligned}
\Sigma_\varepsilon = 
\begin{bmatrix}
\sigma^{2(1)}_\varepsilon & 0 & \cdots & 0\\
0 & \sigma^{2(2)}_\varepsilon\\
\vdots & & \ddots\\
0 & & &\sigma^{2(K)}_\varepsilon
\end{bmatrix},
 \Sigma_\eta =
\begin{bmatrix}
\sigma^{2(1)}_\eta & \sigma^{2(1,2)} & \cdots & \sigma^{2(1, K)}_\eta\\
\sigma^{2(1,2)}_\eta & \sigma^{2(2)}_\eta\\
\vdots & & \ddots\\
\sigma^{2(1, K)}_\eta & & &\sigma^{2(K)}_\eta
\end{bmatrix}
\end{aligned}
\end{equation*}

## Model 3: Correlation is in both Measurement & State eq.

\begin{equation*}
\begin{aligned}
\Sigma_\varepsilon = 
\begin{bmatrix}
\sigma^{2(1)}_\varepsilon & \sigma^{2(1,2)}_\varepsilon & \cdots & \sigma^{2(1, K)}_\varepsilon\\
\sigma^{2(1,2)}_\varepsilon & \sigma^{2(2)}_\varepsilon\\
\vdots & & \ddots\\
\sigma^{2(1, K)}_\varepsilon & & &\sigma^{2(K)}_\varepsilon
\end{bmatrix},
\Sigma_\eta =
\begin{bmatrix}
\sigma^{2(1)}_\eta & \sigma^{2(1,2)} & \cdots & \sigma^{2(1, K)}_\eta\\
\sigma^{2(1,2)}_\eta & \sigma^{2(2)}_\eta\\
\vdots & & \ddots\\
\sigma^{2(1, K)}_\eta & & &\sigma^{2(K)}_\eta
\end{bmatrix}
\end{aligned}
\end{equation*}



## Kalman Filter

For $j$ in $1, 2, ..., J$:
\begin{enumerate}
  \item {Predicted state:} $\boldsymbol{\alpha}_{ij|i(j-1)} = \boldsymbol{\alpha}_{i(j-1)|i(j-1)}$
  \item {Predicted state variance:} $\boldsymbol{P_{ij|i(j-1)} = P_{i(j-1)|i(j-1)}} + \sigma^2_\eta$
  \item {Kalman Gain:} $\boldsymbol{K_{ij}} = \boldsymbol{\boldsymbol{P}_{ij|i(j-1)}(\boldsymbol{P}_{ij|i(j-1)} + \sigma^2_\varepsilon})^{-1}$
  \item {Updated state estimate:} $\boldsymbol{\alpha}_{ij|ij} = \boldsymbol{\alpha}_{ij|i(j-1)} + \boldsymbol{K_{ij}} (\boldsymbol{\tilde y_{ij}- \alpha_{ij|i(j-1)}})$
  \item {Updated state covariance:} $\boldsymbol{P_{ij|ij}} = (1-\boldsymbol{K_{ij}})\boldsymbol{P_{ij|i(j-1)}}$
\end{enumerate}





## Kalman Smoother

For $j^*$ in J, J-1, ..., 1:
\begin{enumerate}
  \item Smoothed predicted state:  $\boldsymbol{\alpha}_{iJ|i(j^*-1)} =\boldsymbol{\alpha}_{i(j^*-1)|i(j^*-1)} + \boldsymbol{L}_{i(j^*-1)} (\boldsymbol{\alpha}_{ij^*|iJ} - \boldsymbol{\alpha}_{ij^*|i(j^*-1)})$
  \item Smoothed predicted state variance:  $\boldsymbol{P_{i(j^*-1)|iJ}} = \boldsymbol{P_{i(j^*-1)|i(j^*-1)}} - \boldsymbol{L_{i(j^*-1)}^2 (P_{ij^*|iJ} -P_{ij^*|i(j^*-1)}})$
\end{enumerate}
Where $\boldsymbol{L_{i(j^*-1)} = P_{{i(j^*-1)}|{i(j^*-1)}}  P^{-1}_{ij^*|i(j^*-1)}}$.

## Backward Sampler

Let $\psi = \{\Sigma_\eta, \Sigma_\varepsilon, \boldsymbol{\beta}\}$,



\begin{equation*}
\begin{aligned}
P_\psi(\boldsymbol{\alpha}_{i(0:J)}|\boldsymbol{y}_{i(1:J)}) &=  P_\psi(\boldsymbol{\alpha}_{iJ}|\boldsymbol{y}_{i(1:J)})P_\psi(\boldsymbol{\alpha}_{i(J-1)}|\boldsymbol{\alpha}_{i(J)}, \boldsymbol{y}_{1:(J-1)}) ...  P_\psi(\boldsymbol{\alpha}_{i0}|\boldsymbol{\alpha}_{i1})
\end{aligned}
\end{equation*}


Therefore we need the following densities for $j$ in $1, 2, ..., J-1$ and $i$ in $1, 2, ..., N$:


\begin{equation*}
\begin{aligned}
P_\psi(\boldsymbol{\alpha}_{ij}|\boldsymbol{\alpha}_{i(j+1)}, \boldsymbol{y}_{i(1:j)}) \propto P_\psi(\boldsymbol{\alpha}_{ij}| \boldsymbol{y}_{i(1:j)})P_\psi(\boldsymbol{\alpha}_{i(j+1)}| \boldsymbol{\alpha}_{ij})
\end{aligned}
\end{equation*}


## Backward Sampler

* From the Kalman Filter we calculate $\boldsymbol{\alpha}_{ij}|\boldsymbol{y}_{i(1:j)} \sim N_\psi(\boldsymbol{\alpha}_{ij|ij}, \boldsymbol{P}_{ij|ij})$ and $\boldsymbol{\alpha}_{i(j+1)}|\boldsymbol{\alpha}_{ij} \sim N_\psi(\boldsymbol{\alpha}_{ij}, \Sigma_\eta)$. 
* After combining the two densities $\boldsymbol{m}_{ij} = E_\psi(\boldsymbol{\alpha}_{ij}| \boldsymbol{\alpha}_{i(j+1)},\boldsymbol{y}_{i(1:j)}) = \boldsymbol{\alpha}_{ij|ij} + \boldsymbol{L}_{ij} (\boldsymbol{\alpha}_{i(j+1)} - \boldsymbol{\alpha}_{i(j+1)|ij})$ and $\boldsymbol{R}_{ij} = \text{Var}_\psi(\boldsymbol{\alpha}_{ij}| \boldsymbol{\alpha}_{i(j+1)},\boldsymbol{y}_{i(1:j)})= \boldsymbol{P}_{ij|ij} - \boldsymbol{L}_{ij}^2 \boldsymbol{P}_{i(j+1)|ij}$. 
* Because of normality, the posterior distribution for $\boldsymbol{\alpha}_{ij}$ is $N(\boldsymbol{m}_{ij}, \boldsymbol{R}_{ij})$.


## $\beta$ Posterior

Has the form


\begin{equation*}
\begin{aligned}
(\boldsymbol{\beta} - \boldsymbol{\Sigma}^{-1} B)'
\boldsymbol{\Sigma}
(\boldsymbol{\beta} - \boldsymbol{\Sigma}^{-1}B)
\end{aligned}
\end{equation*}



Where, $B = \sum^N_{i=1}\sum^T_{j=1}(y_{ij} - \alpha_{ij})'\Sigma_\varepsilon^{-1}\boldsymbol{ \mathbb{X}}_{ij} + \theta\Sigma_{\beta}^{-1}$ and $\boldsymbol{\Sigma} = \sum^N_{i=1}\sum^T_{j=1}\boldsymbol{ \mathbb{X}}_{ij}'\Sigma_\varepsilon^{-1}\boldsymbol{ \mathbb{X}}_{ij} + \Sigma_{\beta}^{-1}p$.

Thus, the posterior for $\beta$ has the following distribution,


\begin{equation*}
\boldsymbol{\beta}| ... \sim N(\Sigma^{-1}B, \Sigma^{-1})
\end{equation*}



## $\Sigma_\varepsilon$ Posterior

\begin{align*}
P&(\Sigma_\varepsilon|Y, \boldsymbol{\alpha}, \Sigma_\eta, \boldsymbol{\beta})  = P(Y, \boldsymbol{\alpha}, \Sigma_\eta, \boldsymbol{\beta}|\Sigma_\varepsilon)P(\Sigma_\varepsilon) \\ 
&\propto |\Sigma_\varepsilon|^{-(NJ +\nu_\varepsilon+p+1)/2}\text{exp}(-tr(\Lambda_\varepsilon + \sum^N_{i = 1}\sum^J_{j= 1} (\boldsymbol{y}_{ij} - \boldsymbol{\alpha}_{ij} - \boldsymbol{ \mathbb{X}}_{ij}\boldsymbol{\beta})^2)\Sigma_\varepsilon^{-1})\\
\Sigma_\varepsilon| ... & \sim \text{Inv-Wishart}_{\nu_\varepsilon + NJ}(\Lambda_\varepsilon+\sum^N_{i = 1}\sum^J_{j= 1} (\boldsymbol{y}_{ij} - \boldsymbol{\alpha}_{ij} - \boldsymbol{ \mathbb{X}}_{ij}\boldsymbol{\beta})^2)
\end{align*}

The posterior for the diagnol matrix $\Sigma_\varepsilon$ can be calculated independently for each test as was previously shown [Paper1] using the inverse-gamma for each test.



## $\Sigma_\eta$ Posterior

\begin{align*}
P&(\Sigma_\eta|Y, \boldsymbol{\alpha}, \Sigma_\eta, \boldsymbol{\beta})  = P(Y, \boldsymbol{\alpha}, \Sigma_\eta, \boldsymbol{\beta}|\Sigma_\eta)P(\Sigma_\eta) \\ 
&\propto |\Sigma_\eta|^{-(N(J-1) +\nu_\eta+p+1)/2}\text{exp}(-tr(\Lambda_\eta + \sum^N_{i = 1}\sum^J_{j= 2} (\boldsymbol{\alpha}_{ij} - \boldsymbol{\alpha}_{i(j-1)})^2)\Sigma_\eta^{-1})\\
\Sigma_\eta| ...  & \sim \text{Inv-Wishart}_{\nu_\eta + N(J-1)}(\Lambda_\eta+\sum^N_{i = 1}\sum^J_{j= 2} (\boldsymbol{\alpha}_{ij} - \boldsymbol{\alpha}_{i(j-1)})^2)
\end{align*}

## $\beta$ Standardization

\begin{equation*}
\begin{aligned}
\boldsymbol{y}_{ij} &= \boldsymbol{\alpha}_{ij}^{(m)} + \boldsymbol{ \mathbb{X}}_{ij} \boldsymbol{\beta}^{(m)} + \boldsymbol{\varepsilon}_{ij}\\
\boldsymbol{y}_{ij}-\boldsymbol{\alpha}_{ij}^{(m)} &=   \boldsymbol{ \mathbb{X}}_{ij} \boldsymbol{\beta}^{(m)} + \boldsymbol{\varepsilon}_{ij}\\
(\boldsymbol{y}_{ij}-\boldsymbol{\alpha}_{ij}^{(m)})(\Sigma_{\varepsilon}^{(m)})^{-1/2} &=   \boldsymbol{ \mathbb{X}}_{ij} \boldsymbol{\beta}^{(m)}(\Sigma_{\varepsilon}^{(m)})^{-1/2} + \boldsymbol{\varepsilon}_{ij}(\Sigma_{\varepsilon}^{(m)})^{-1/2}\\
\boldsymbol{y}_{ij}^{(m)*} &= \boldsymbol{ \mathbb{X}}_{ij} \boldsymbol{\beta}^{(m)}(\Sigma_{\varepsilon}^{(m)})^{-1/2} + \boldsymbol{\varepsilon}_{ij}^*
\end{aligned}
\end{equation*}

Now, var($\boldsymbol{\varepsilon}_{ij}^*$) = $I$. Each test is now independent and effect comparison can be made using $\boldsymbol{\beta}^{*(m)} = \boldsymbol{\beta}^{(m)}(\Sigma_{\varepsilon}^{(m)})^{-1/2}$.


## Fully Simulated Data Analysis

We simulated 100 subjects with between 2-13 observations from the following model:


\begin{equation*}
\begin{aligned}
\begin{bmatrix}
y_{ij1}\\
y_{ij2}\\
y_{ij3}
\end{bmatrix}
&= \begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\alpha_{ij3}
\end{bmatrix}
+ 
\begin{bmatrix}
\boldsymbol{x_{ij}\beta_1}\\
\boldsymbol{x_{ij}\beta_2}\\
\boldsymbol{x_{ij}\beta_3} 
\end{bmatrix} +
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\varepsilon_{ij3}
\end{bmatrix},  
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\varepsilon_{ij3}
\end{bmatrix} 
\sim N(0, 
\begin{bmatrix}
15 & 2.4 & 1\\
2.4 & 15 & 1\\
1 & 1 & 10
\end{bmatrix}
)\\
\begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\alpha_{ij3}
\end{bmatrix} & = 
\begin{bmatrix}
\alpha_{i(j-1)1}\\
\alpha_{i(j-1)2}\\
\alpha_{i(j-1)3}
\end{bmatrix} +
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\eta_{ij3}
\end{bmatrix}, 
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\eta_{ij3}
\end{bmatrix} \sim N(0, \delta_{ij}
\begin{bmatrix}
5 & 3.7 & 0\\
3.7 & 5 & 0\\
0 & 0 & 2
\end{bmatrix}
)
\end{aligned}
\end{equation*}



```{r}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
path = "FJCosTESTfullchain10"

sigeps <- c(15, 15, 10)
SIGeps <- diag(sigeps)
SIGeps[1,2] <- SIGeps[2, 1] <- 2.4
SIGeps[1,3] <- SIGeps[3, 1] <- 1
SIGeps[2,3] <- SIGeps[3, 2] <- 1

Sigma <- c(5, 3.714281, 0, 3.714281, 5, 0, 0, 0, 2) %>%
  matrix(3, 3)

```


## Parameter coverage

```{r, fig.height=6}
fse <- readRDS(paste("Paper2/paperData/",path, "_Fake_Effects.RDS", sep = "")) %>%
  map2(names(.), ~cbind(.x, method = .y) %>% mutate(beta = c(4, 2, 1, -3, 0, 1, 0, 0, 0))) %>%
  do.call("rbind", .) %>%
  mutate(
    Variable = case_when(
      Beta %in% paste("B", c(1, 4, 7), sep = "") ~ "X1",
      Beta %in% paste("B", c(1, 4, 7)+1, sep = "") ~ "X2",
      Beta %in% paste("B", c(1, 4, 7)+2, sep = "") ~ "X3"
    ), 
    Test = case_when(
      Beta %in% paste("B", 1:3, sep = "") ~ "Y1",
      Beta %in% paste("B", (1:3) + 3, sep = "") ~ "Y2",
      Beta %in% paste("B", (1:3) + 6, sep = "") ~ "Y3"
    ),
    BLCL = LCL - (AvgMean - avgBias),
    BUCL = UCL - (AvgMean - avgBias),
    Method = case_when(
      method == "Dub" ~ "OS",
      method == "St" ~ "S",
      method == "Obs" ~ "O",
      method == "Ind" ~ "LLT",
    ) %>%
      factor(levels = c("LLT", "O", "S", "OS")),
    Coverage = round(Coverage, 3)
  )

fse %>%
  select(Test, Variable, Beta = beta, Coverage, Method) %>%
  spread(Method, Coverage) %>%
  mutate(across(.cols = LLT:OS, ~paste( round(100 * .x, 1), "%", sep = ""))) %>%
  kbl(row.names = FALSE,  booktabs = TRUE, caption = "Linear effect coverage percentage.")

```



## Bias and Variability

```{r, fig.height=6}
fse %>%
  ggplot(aes(x = Method, y = avgBias, color = Method, shape = Method)) +
  geom_hline(yintercept = 0) + 
  geom_point(size = 5) +
  geom_errorbar(aes(ymin = BLCL, ymax = BUCL)) +
  facet_grid(Test ~ Variable) +
  scale_color_manual(values = c("#405E92",   "#DF3116",     "#DFB916", "#409247"))  +
  scale_shape_manual(values = c(15,10,  9, 8))  +
  ylab("Avg Bias")  +
  theme_bw()
```





## Variance Coverage

```{r}
## Fully Simulated Correlated Measurement Error
EpsL <- readRDS(paste("Paper2/paperData/",path, "_Fake_Eps.RDS", sep = ""))

EpsLdf <- EpsL %>%
  map("avg") %>%
  map(~.x[!lower.tri(.x)]) %>%
  do.call("cbind", .) %>%
  cbind(true = SIGeps[!lower.tri(SIGeps)]) %>%
  round(3) %>%
  data.frame

EpsLdf$param = (param = outer(1:3, 1:3, FUN = "paste", sep = ",")[!lower.tri(SIGeps)])
EpsClose <- EpsLdf



## Fully Simulated Correlated Measurement Error Coverage
EpsLdfcov <- EpsL %>%
  map("coverage") %>%
  map(~.x[!lower.tri(.x)]) %>%
  do.call("cbind", .) %>%
  round(3) %>%
  data.frame




for( i in colnames(EpsLdfcov)){
  EpsLdf[[i]] <- paste(EpsLdf[[i]], " (", round(100 * EpsLdfcov[[i]], 1), "%)", sep = "")
}


EpsLdf$st[c(2, 4, 5)] <- EpsLdf$ind[c(2, 4, 5)] <- '-'





## Fully Simulated Correlated Measurement Error
EtaL <- readRDS(paste("Paper2/paperData/",path, "_Fake_Eta.RDS", sep = ""))

EtaLdf <- EtaL %>%
  map("avg") %>%
  map(~.x[!lower.tri(.x)]) %>%
  do.call("cbind", .) %>%
  cbind(true = Sigma[!lower.tri(Sigma)]) %>%
  round(3) %>%
  data.frame

EtaLdf$param = (param = outer(1:3, 1:3, FUN = "paste", sep = ",")[!lower.tri(Sigma)])
EtaClose <- EtaLdf



## Fully Simulated Correlated Measurement Error Coverage
EtaLdfcov <- EtaL %>%
  map("coverage") %>%
  map(~.x[!lower.tri(.x)]) %>%
  do.call("cbind", .) %>%
  round(3) %>%
  data.frame




for( i in colnames(EtaLdfcov)){
  EtaLdf[[i]] <- paste(EtaLdf[[i]], " (", round(100 * EtaLdfcov[[i]], 1), "%)", sep = "")
}


EtaLdf$obs[c(2, 4, 5)] <- EtaLdf$ind[c(2, 4, 5)] <- '-'

rbind(mutate(EpsLdf, Error = "Observation Error"), mutate(EtaLdf, Error = "State Process")) %>%
  mutate_all(.funs = list(function(x)gsub("100%", "-", x))) %>%
  select(Error, Parameter = param, `True value` = true,  LLT = ind, O = obs, S = st, OS = dub) %>%
  mutate(Error = factor(Error)) %>%
  kbl(row.names = FALSE,  booktabs = TRUE, align = "c") %>%
  collapse_rows(1:2, row_group_label_position = 'stack') %>%
  kable_styling(font_size = 6)



```

\raggedright


## Real Data Simulation

## Simulated Parameter Coverage

```{r, include = FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(igraph)
library(ggtext)
varorder <- c('LOGIMEM', 'MEMUNITS', 'DIGIF', 'DIGIB', 'WAIS', 'TRAILA', 'TRAILB', 'BOSTON', 'ANIMALS', 'VEG')
```


```{r}
readRDS("Paper2/paperData/Real_Effect_Coverage.RDS") %>%
  map(function(x){
    out <- x$Coverage
    names(out) <- x$Outcome
    out
  }) %>%
  data.frame() %>%
  select(LLT = Independent, O = Observation, S = State, OS = Both) %>%
  mutate_all(~paste( round(100 * .x, 1), "%", sep = "")) %>%
  kbl(row.names = TRUE,  booktabs = TRUE)
```



## Simulated Parameter Bias and Variability

```{r, fig.height = 6}
readRDS("Paper2/paperData/Real_effects.RDS")  %>%
  mutate(Method = case_when(
    Method == "Independent" ~ "LLT",
    Method == "Observation" ~ "O",
    Method == "State" ~ "S",
    Method == "Both" ~ "OS"
  ) %>% factor(levels = c("LLT", "O", "S", "OS"))) %>%
  ggplot(aes(x = Method, y = Estimate, color = Method, shape = Method)) +
  geom_point(size=5) +
  geom_errorbar(aes(ymin = LCL, ymax = UCL), size = 1) +
  facet_wrap(~Outcome) +
  geom_hline(yintercept = 1, col = "red") +
  scale_color_manual(values = c("#405E92",   "#DF3116",     "#DFB916", "#409247"))  +
  scale_shape_manual(values = c(15,10,  9, 8))  +
  ylab("Avg Bias")  +
  theme_bw()
```




## Covariance Matrices


```{r, fig.height = 6}


rbind(
  readRDS("Paper2/paperData/Real_Eps.RDS") %>%
    mutate(Status = "Observation"),
  readRDS("Paper2/paperData/Real_Eta.RDS") %>%
    mutate(Status = "State")
) %>%
  mutate(V1 = factor(V1, levels = varorder), V2 = factor(V2, levels = varorder)) %>%
  ggplot(aes(x = V2, y = V1, fill = COR, label = print_values)) +
  xlab("") +
  ylab("") +
  theme_minimal()  +
  geom_tile( color = 'white')+
  geom_richtext(
        fill = NA, label.color = NA, # remove background and outline
        label.padding = grid::unit(rep(0, 4), "pt") # remove padding
  )  +
  scale_fill_gradient2(low = 'lightblue', high = "red", 
    midpoint = .5,limits = c(0,1), na.value = "lightblue",
    name="Correlation")+
  facet_wrap("Status") +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(angle = -30, vjust = 0, hjust=.05)
    # legend.justification = c(1, 0),
    # legend.position = c(0.4, 0.6),
    # legend.direction = "horizontal"
    )
```


## Data Analysis


## Dementia Transitioners

```{r, fig.height = 6}
rbind(
  readRDS("Paper2/paperData/NACC_Eps_demen.RDS") %>%
    mutate(Status = "Observation"),
  readRDS("Paper2/paperData/NACC_Eta_demen.RDS") %>%
    mutate(Status = "State")
) %>%
  mutate(V1 = factor(V1, levels = varorder), V2 = factor(V2, levels = varorder)) %>%
  ggplot(aes(x = V2, y = V1, fill = COR, label = print_values)) +
  xlab("") +
  ylab("") +
  theme_minimal()  +
  geom_tile( color = 'white')+
  geom_richtext(
        fill = NA, label.color = NA, # remove background and outline
        label.padding = grid::unit(rep(0, 4), "pt") # remove padding
  )  +
  scale_fill_gradient2(low = 'lightblue', high = "red", 
    midpoint = .5,limits = c(0,1), na.value = "lightblue",
    name="Correlation")+
  facet_wrap("Status") +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(angle = -30, vjust = 0, hjust=.05)
    # legend.justification = c(1, 0),
    # legend.position = c(0.4, 0.6),
    # legend.direction = "horizontal"
    ) +
  # guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
  #               title.position = "top", title.hjust = 0.5)) +
  ggtitle("Dementia")

```



## Non-Dementia Transitioners

```{r, fig.height = 6}
rbind(
  readRDS("Paper2/paperData/NACC_Eps_non_demen.RDS") %>%
    mutate(Status = "Observation"),
  readRDS("Paper2/paperData/NACC_Eta_non_demen.RDS") %>%
    mutate(Status = "State")
) %>%
  mutate(V1 = factor(V1, levels = varorder), V2 = factor(V2, levels = varorder)) %>%
  ggplot(aes(x = V2, y = V1, fill = COR, label = print_values)) +
  xlab("") +
  ylab("") +
  theme_minimal()  +
  geom_tile( color = 'white')+
  geom_richtext(
        fill = NA, label.color = NA, # remove background and outline
        label.padding = grid::unit(rep(0, 4), "pt") # remove padding
  )  +
  scale_fill_gradient2(low = 'lightblue', high = "red", 
    midpoint = .5,limits = c(0,1), na.value = "lightblue",
    name="Correlation")+
  facet_wrap("Status") +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(angle = -30, vjust = 0, hjust=.05)
    # legend.justification = c(1, 0),
    # legend.position = c(0.4, 0.6),
    # legend.direction = "horizontal"
    ) +
  # guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
  #               title.position = "top", title.hjust = 0.5)) +
  ggtitle("Non-Dementia")

```



# Project 3 Overview

## Project 3 Aims

* We wish to better measure the underlying relatedness between cognition tests and how they load to certain domains.
* We propose the use of an LLT factor analysis.

## Project 3


\begin{equation*}
\begin{aligned}
\begin{bmatrix}
y_{ij1}\\
y_{ij2}\\
\vdots\\
y_{ijK}
\end{bmatrix}
&= G \begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\vdots\\
\alpha_{ijq}
\end{bmatrix}
+ 
\begin{bmatrix}
\boldsymbol{x_{ij}\beta^{(1)}}\\
\boldsymbol{x_{ij}\beta^{(2)}}\\
\vdots\\
\boldsymbol{x_{ij}\beta^{(K)}} 
\end{bmatrix} +
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\vdots\\
\varepsilon_{ijK}
\end{bmatrix},  
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\vdots\\
\varepsilon_{ijK}
\end{bmatrix} 
\sim N(0, \sigma^2_\varepsilon I
)\\
\begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\vdots\\
\alpha_{ijq}
\end{bmatrix} & = 
\begin{bmatrix}
\alpha_{i(j-1)1}\\
\alpha_{i(j-1)2}\\
\vdots\\
\alpha_{i(j-1)q}
\end{bmatrix} +
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\vdots\\
\eta_{ijq}
\end{bmatrix}, 
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\vdots\\
\eta_{ijq}
\end{bmatrix} \sim N(0, \delta_{ij}\sigma^2_\eta I)
\end{aligned}
\end{equation*}


Where $G \in R^{K\times q}$ is a factor loading matrix. We will again model this in the Bayesian context and give each entry of G a normal prior ($G_{s,t} \sim N(0, \sigma^2_G)$).


## Project 3

\begin{equation*}
\begin{aligned}
\begin{bmatrix}
y_{ij1}\\
y_{ij2}\\
\vdots\\
y_{ijK}
\end{bmatrix}
&= G \begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\vdots\\
\alpha_{ijK}
\end{bmatrix}
+ 
\begin{bmatrix}
\boldsymbol{x_{ij}\beta^{(1)}}\\
\boldsymbol{x_{ij}\beta^{(2)}}\\
\vdots\\
\boldsymbol{x_{ij}\beta^{(K)}} 
\end{bmatrix} +
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\vdots\\
\varepsilon_{ijK}
\end{bmatrix},  
\begin{bmatrix}
\varepsilon_{ij1}\\
\varepsilon_{ij2}\\
\vdots\\
\varepsilon_{ijK}
\end{bmatrix} 
\sim N(0, \sigma^2_\varepsilon I
)\\
\begin{bmatrix}
\alpha_{ij1}\\
\alpha_{ij2}\\
\vdots\\
\alpha_{ijK}
\end{bmatrix} & = 
\begin{bmatrix}
\alpha_{i(j-1)1}\\
\alpha_{i(j-1)2}\\
\vdots\\
\alpha_{i(j-1)K}
\end{bmatrix} +
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\vdots\\
\eta_{ijK}
\end{bmatrix}, \
\begin{bmatrix}
\eta_{ij1}\\
\eta_{ij2}\\
\vdots\\
\eta_{ijK}
\end{bmatrix} \sim \tau N(0, \delta_{ij}\sigma^2_\eta I) + (1-\tau) N(0, w)
\end{aligned}
\end{equation*}

Where $G$ is now a $K\times K$ matrix and each chain of $\alpha$ has a spike and slab distribution. The vector $\tau$ is of length $K$ and controls which chains of $\alpha$ will be set to 0. Using this spike and slab we are able to see how the underlying cognition levels naturally cluster together.



## Timeline

* Project 1 is finished.
* Start coding project 3 immediately while finishing a write up for project 2 by the end of March.
* Finish project 3 in June/July.
* Defend September.


## Thank you!

* Recommendations?
* Questions?
 